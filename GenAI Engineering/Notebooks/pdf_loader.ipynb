{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From PDFs to Powerful Answers**\n",
    "\n",
    "What if a language model could not only read a dense research paper, but answer your questions about it â€” intelligently and in context? This project demonstrates exactly that, combining `IBM Watsonx` `LLMs` and `LangChain` to build a `retrieval-augmented system` that makes complex documents searchable, chunkable, and explorable via `natural language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAIN STRUCTURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# included to maintain privacy on my Watson \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.0 | PDF Ingestion**\n",
    "\n",
    "This task fetches and loads a PDF into LangChain for downstream text analysis.\n",
    "\n",
    "**Ingesting Academic Papers from the Web using LangChain**\n",
    "\n",
    "*Download and load a PDF into a LangChain document object for structured processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
      "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
      "inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some-\n",
      "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
      "BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications. All relevant code an\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf\"\n",
    "pdf_path = \"lora_review.pdf\"\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(pdf_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.0 | Chunking Raw LaTeX with LangChain**\n",
    "\n",
    "This task simulates real-world preprocessing by chunking LaTeX content into manageable sections.\n",
    "\n",
    "**Chunking Raw LaTeX Content for LLM Readability**\n",
    "\n",
    "*Transforming LaTeX text into clean document chunks using recursive splitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "\\documentclass{article}\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\\section{Introduction}\n",
      "Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\\subsection{History of LLMs}\n",
      "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "\\subsection{Applications of LLMs}\n",
      "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "latex_text = \"\"\"\n",
    "\\\\documentclass{article}\n",
    "\\\\begin{document}\n",
    "\\\\maketitle\n",
    "\\\\section{Introduction}\n",
    "Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
    "\n",
    "\\\\subsection{History of LLMs}\n",
    "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
    "\n",
    "\\\\subsection{Applications of LLMs}\n",
    "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
    "\\\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "# Wrap LaTeX as a LangChain Document\n",
    "doc = Document(page_content=latex_text)\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Perform the split\n",
    "split_docs = splitter.split_documents([doc])\n",
    "\n",
    "# Display results\n",
    "for i, d in enumerate(split_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{d.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.0 | Generating Embeddings with HuggingFace**\n",
    "\n",
    "This task demonstrates how to convert queries into dense vector embeddings\n",
    "\n",
    "**Embedding Natural Language Queries Using HuggingFace Transformers**\n",
    "\n",
    "*Utilizes the MiniLM model to convert input text into a high-dimensional embedding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rand Sobczak Jr\\AppData\\Local\\Temp\\ipykernel_19828\\4274722075.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 embedding values: [0.007003897335380316, 0.010914130136370659, 0.08746254444122314, 0.086799256503582, 0.02664852701127529]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load pre-trained embedding model (can replace with any local or HF model you prefer)\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"How are you?\"\n",
    "\n",
    "embedding_result = embedder.embed_query(query)\n",
    "\n",
    "print(\"First 5 embedding values:\", embedding_result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 | Secure Credential Handling with `getpass()`**\n",
    "\n",
    "This task securely captures sensitive IBM Watsonx credentials and loads them into environment variables for API access\n",
    "\n",
    "**Protecting API Credentials with getpass() for IBM Watsonx**\n",
    "\n",
    "*Use Pythonâ€™s `getpass()` to safely manage API keys and project configs without exposing them in source code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your Watsonx API Key: \")\n",
    "project_id = getpass(\"Enter your Watsonx Project ID: \")\n",
    "w_url = getpass(\"Enter your Watsonx URL: \")\n",
    "\n",
    "os.environ[\"WATSONX_APIKEY\"] = api_key\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = project_id\n",
    "os.environ[\"WATSONX_URL\"] = w_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.0 | Vectorizing Policies with Watsonx Embeddings**\n",
    "\n",
    "This task downloads a text policy file, chunks it, embeds it using IBM Watsonx, and enables semantic search with LangChain & Chroma\n",
    "\n",
    "**Embedding and Searching Policy Texts with IBM Watsonx + Chroma**\n",
    "\n",
    "*Download policy documents, split them for retrieval, embed with Watsonx, and perform similarity search queries.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "This policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\n",
      "\n",
      "Result 2:\n",
      "This policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\n",
      "\n",
      "Result 3:\n",
      "Consequences: Violations of this policy may lead to disciplinary action, including potential termination.\n",
      "\n",
      "This policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\n",
      "\n",
      "4. Mobile Phone Policy\n",
      "\n",
      "Result 4:\n",
      "Consequences: Violations of this policy may lead to disciplinary action, including potential termination.\n",
      "\n",
      "This policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\n",
      "\n",
      "4. Mobile Phone Policy\n",
      "\n",
      "Result 5:\n",
      "Employee Referrals: We welcome employee referrals as they help build a strong and engaged team.\n",
      "\n",
      "This policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\n",
      "\n",
      "\n",
      "3. Internet and Email Policy\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ibm.embeddings import WatsonxEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\"\n",
    "local_path = \"new-Policies.txt\"\n",
    "if not os.path.exists(local_path):\n",
    "    import requests\n",
    "    response = requests.get(url)\n",
    "    with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "loader = TextLoader(local_path)\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=os.environ[\"WATSONX_URL\"],\n",
    "    apikey=os.environ[\"WATSONX_APIKEY\"],\n",
    "    project_id=os.environ[\"WATSONX_PROJECT_ID\"]\n",
    ")\n",
    "\n",
    "vector_db = Chroma.from_documents(split_docs, embedding, persist_directory=\"./vectorstore\")\n",
    "\n",
    "query = \"Smoking policy\"\n",
    "results = vector_db.similarity_search(query, k=5)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\\n{doc.page_content[:500]}\")  # Print only first 500 chars per result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.0 | Creating a Retriever for Targeted Search**\n",
    "\n",
    "This task builds a retriever from embedded policy documents using IBM Watsonx and Chroma, enabling precise semantic lookup.\n",
    "\n",
    "**Building a Custom Retriever with IBM Watsonx & Chroma**\n",
    "\n",
    "*Convert text into searchable vector representations and query for specific policies using a retriever interface.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Email policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rand Sobczak Jr\\AppData\\Local\\Temp\\ipykernel_19828\\737401652.py:33: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "3. Internet and Email Policy\n",
      "\n",
      "Our Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\n",
      "\n",
      "Acceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\n",
      "\n",
      "Result 2:\n",
      "3. Internet and Email Policy\n",
      "\n",
      "Our Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\n",
      "\n",
      "Acceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# Load document\n",
    "loader = TextLoader(\"new-Policies.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=os.environ[\"WATSONX_URL\"],\n",
    "    apikey=os.environ[\"WATSONX_APIKEY\"],\n",
    "    project_id=os.environ[\"WATSONX_PROJECT_ID\"]\n",
    ")\n",
    "\n",
    "# Use Chroma to create a retriever\n",
    "vector_db = Chroma.from_documents(split_docs, embedding, persist_directory=\"./vectorstore_task5\")\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# Run the retriever\n",
    "query = \"Email policy\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Display the results\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\\n{res.page_content[:500]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.0 | Retrieval-Augmented Generation with IBM Watsonx**\n",
    "\n",
    "This task demonstrates a full RAG pipeline using Watsonx embeddings and Granite LLMs to answer natural language queries grounded in a research paper.\n",
    "\n",
    "**RAG Pipeline with IBM Watsonx: Grounded QA Over Research PDFs**\n",
    "\n",
    "*Combines document embeddings and Granite LLM to answer user queries with evidence-based responses using LangChainâ€™s RetrievalQA.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rand Sobczak Jr\\AppData\\Local\\Temp\\ipykernel_19828\\1232616504.py:48: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Query: Is eating cheese healthy?\n",
      "ðŸ§  Answer:  Yes, cheese is a great source of calcium and protein. It can also help you maintain a healthy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ibm.embeddings import WatsonxEmbeddings\n",
    "from langchain_ibm.llms import WatsonxLLM\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf\"\n",
    "pdf_path = \"review-paper.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",  \n",
    "    url=os.environ[\"WATSONX_URL\"],\n",
    "    apikey=os.environ[\"WATSONX_APIKEY\"],\n",
    "    project_id=os.environ[\"WATSONX_PROJECT_ID\"]\n",
    ")\n",
    "\n",
    "vector_db = Chroma.from_documents(split_docs, embedding, persist_directory=\"./rag_bot_vectorstore\")\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\", \n",
    "    url=os.environ[\"WATSONX_URL\"],\n",
    "    apikey=os.environ[\"WATSONX_APIKEY\"],\n",
    "    project_id=os.environ[\"WATSONX_PROJECT_ID\"]\n",
    ")\n",
    "\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=False)\n",
    "\n",
    "query = \"Is eating cheese healthy?\"\n",
    "answer = qa_chain.run(query)\n",
    "\n",
    "print(\"\\nðŸ”Ž Query:\", query)\n",
    "print(\"ðŸ§  Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
