
In Generative AI with Large Language Models (LLMs), the fundamentals of how generative AI works were learn, and how to deploy it in real-world applications.

By taking this course, I learned to:
- Deeply understand generative AI, describing the key steps in a typical LLM-based generative AI lifecycle, from data gathering and model selection, to performance evaluation and deployment
- Describe in detail the transformer architecture that powers LLMs, how theyâ€™re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases
- Use empirical scaling laws to optimize the model's objective function across dataset size, compute budget, and inference requirements
- Apply state-of-the art training, tuning, inference, tools, and deployment methods to maximize the performance of models within the specific constraints of your project 
- Discuss the challenges and opportunities that generative AI creates for businesses after hearing stories from industry researchers and practitioners

This is an intermediate course, those to take the course should have, which I did, some experience coding in Python to get the most out of it. You should also be familiar with the basics of machine learning, such as supervised and unsupervised learning, loss functions, and splitting data into training, validation, and test sets.

