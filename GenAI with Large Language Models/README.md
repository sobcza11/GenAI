![alt text](https://github.com/sobcza11/GenAI/blob/main/GenAI%20with%20Large%20Language%20Models/_support/aws_deeplearning.jpg)
# OVERVIEW
In the Generative AI with Large Language Models (LLMs) course, I gained a comprehensive understanding of the fundamental principles behind generative AI and how to effectively deploy it in real-world applications. Through this course, I acquired the ability to:

<b>- Comprehend the generative AI lifecycle:</b> I developed a deep understanding of the key stages involved in an LLM-based generative AI workflowâ€”ranging from data collection and model selection to performance evaluation and deployment.

<b>- Analyze transformer architecture:</b> I learned the intricate details of the transformer architecture that underpins LLMs, including their training processes and how fine-tuning enables adaptation to diverse, domain-specific use cases.

<b>- Optimize model performance:</b> I applied empirical scaling laws to optimize the model's objective function by balancing dataset size, computational resources, and inference efficiency.
Implement advanced techniques:</b> I explored state-of-the-art methods for training, tuning, inference, and deployment, ensuring model performance is maximized while meeting project-specific constraints.

<b>- Evaluate business implications:</b> I assessed the opportunities and challenges generative AI presents for businesses, informed by insights from industry experts and real-world case studies.

This intermediate-level course required prior experience in Python programming, which I possessed, along with a solid foundation in machine learning concepts, including supervised and unsupervised learning, loss functions, and the partitioning of data into training, validation, and test sets.

